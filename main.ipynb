{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4n5wpcM3Xlq"
   },
   "source": [
    "# Team info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGFaadWN3snq"
   },
   "source": [
    "# Project info\n",
    "\n",
    "Develop a model to detect and classify breast cancer from histopathology images.\n",
    "\n",
    "goals and bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKqliN0ThK8S"
   },
   "source": [
    "### 0.1. Explanation of medical terms used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eeuc-lgK3v3h"
   },
   "source": [
    "## 1. Data exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIDfYumLfx6K"
   },
   "source": [
    "### 1.1. Import Data and adjust paths (I suggest to work on Kaggle) [Andrea + Mattia]\n",
    "\n",
    "See kaggle notebook, this part it's just copy pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBBEzwxj32s3"
   },
   "source": [
    "### 1.2.1. Preprocessing [Andrea + Mattia]\n",
    "Clean data as input for next tasks.\n",
    "\n",
    "Feature engineering\n",
    "\n",
    "Hint: copy a dataset until now to give it to Asia for Visualization only (categorical variables can't be rappresented with further data engineering)\n",
    "\n",
    "After visualization, transform categorical variables in dummies\n",
    "\n",
    "Also map target feature into integer value\n",
    "\n",
    "Hint: Thi new df is usefull for modelling and for corr matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "calc_train_path = \"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/calc_case_description_train_set.csv\"\n",
    "calc_test_path = \"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/calc_case_description_test_set.csv\"\n",
    "mass_train_path = \"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_train_set.csv\"\n",
    "mass_test_path = \"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_test_set.csv\"\n",
    "\n",
    "# Datasets\n",
    "calc_train = pd.read_csv(calc_train_path)\n",
    "calc_test = pd.read_csv(calc_test_path)\n",
    "mass_train = pd.read_csv(mass_train_path)\n",
    "mass_test = pd.read_csv(mass_test_path)\n",
    "\n",
    "calc_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing datasets: calc_train, calc_test, mass_train, mass_test\n",
    "\n",
    "# Step 1: Clean the datasets (remove rows with missing values)\n",
    "calc_test_cleaned = calc_test.copy().dropna()\n",
    "calc_train_cleaned = calc_train.copy().dropna()\n",
    "mass_train_cleaned = mass_train.copy().dropna()\n",
    "mass_test_cleaned = mass_test.copy().dropna()\n",
    "\n",
    "\n",
    "# Step 2: Transform categorical variables into dummy variables\n",
    "calc_test_transformed = pd.get_dummies(calc_test_cleaned, drop_first=True)\n",
    "calc_train_transformed = pd.get_dummies(calc_train_cleaned, drop_first=True)\n",
    "mass_train_transformed = pd.get_dummies(mass_train_cleaned, drop_first=True)\n",
    "mass_test_transformed = pd.get_dummies(mass_test_cleaned, drop_first=True)\n",
    "\n",
    "# Step 3: Map target feature (`pathology`) to integers for binary classification\n",
    "calc_train_transformed['pathology'] = calc_train_cleaned['pathology'].apply(\n",
    "   lambda x: 1 if x == 'MALIGNANT' else 0)\n",
    "calc_test_transformed['pathology'] = calc_test_cleaned['pathology'].apply(\n",
    "   lambda x: 1 if x == 'MALIGNANT' else 0)\n",
    "mass_train_transformed['pathology'] = mass_train_cleaned['pathology'].apply(\n",
    "    lambda x: 1 if x == 'MALIGNANT' else 0)\n",
    "mass_test_transformed['pathology'] = mass_test_cleaned['pathology'].apply(\n",
    "    lambda x: 1 if x == 'MALIGNANT' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6FINL4rfvIi"
   },
   "source": [
    "### 1.3. EDA + Visualization [Asia]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keboOJPSgoiK"
   },
   "source": [
    "### 1.3.1. Hint: Use PCA on target variables to understand if they overlap and how well they separate to have a spoiler of how well our data can be labeled and how well our models can perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3faZPV0Pkf9u"
   },
   "source": [
    "Focus on distribution, using boxplot or histogram or whatever\n",
    "- plot any categorical variables\n",
    "- plot any numerical\n",
    "- plot target\n",
    "\n",
    "Focus on target var\n",
    "- how each feature may change given that it s labeled with a certain target feature\n",
    "- any statistical analysis usefull\n",
    "\n",
    "Focus on correlation\n",
    "- corr matrix seaborn\n",
    "\n",
    "More\n",
    "- others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bq2iun3hfQ0"
   },
   "source": [
    "### 1.4 Testing Class Imbalance Tecniques [idk]\n",
    "\n",
    "- F Beta Score with differents Beta Value\n",
    "- AUC\n",
    "- Confusion Matrix\n",
    "- others\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFU_4OTH3_jB"
   },
   "source": [
    "## 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaBjqdNzmvW-"
   },
   "source": [
    "### 2.0.0. Data preparation for modeling\n",
    "- Split\n",
    "- idk if we can do CV here, if we can we need to find best tecnique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP5XkRT8hzDz"
   },
   "source": [
    "### 2.0.1 Data Augmentation tecniques [Vika]\n",
    "\n",
    "Test on CNN architecture structure to balance a good trade off.\n",
    "A lot of Data Augmentation may cause overfitting, if possible use it to make the dataset bigger by giving more input to model idk.\n",
    "When u add more pay attention to dropout feature in CNN, a greater value (0.5 for instance) may overfit with more data augmentation tecniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsObY5-r4ifv"
   },
   "source": [
    "### 2.1. CNN [Vika]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cu1WWygKlDN0"
   },
   "source": [
    "### 2.1.1. ?? Merging with more datas can give us better results?? [Vika]\n",
    " if u have time, merge dataset woth other similar datasets (see kaggle notebook) so u have greater input\n",
    "Check how same structure works on same test set when\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFCRZRyClNWz"
   },
   "source": [
    "### 2.2.1 Premade CNN 1 [Andrea]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3UoUiY-lRoF"
   },
   "source": [
    "### 2.2.2 Premade CNN 2 [Mattia]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljuWxxOcgIgS"
   },
   "source": [
    "### 2.3. Mergin unstructured and structured data (imgs + metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQ7kzQ9n4naM"
   },
   "source": [
    "### 2.4. Boosting Methods [Em]\n",
    "Do we have a lot of models? depends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6A84nZnV4r0n"
   },
   "source": [
    "### 2.2.1. Embedding Methods [Em]\n",
    "- what the hell is that? Use more boosting methods together to achieve better results\n",
    "- as I get it is semi important. Nah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smZHfUp9-L_2"
   },
   "source": [
    "## 3. Dimensionallity Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJu1oLSi-ghH"
   },
   "source": [
    "### 3.1. Principal Component Analysis (PCA) [Asia]\n",
    "\n",
    "Not needed, we may use autoencoders actually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRUihcC0-sNX"
   },
   "source": [
    "### 3.2. t-Distributed Stochastic Neighbor Embedding [Vika]\n",
    "to check if we really need it\n",
    "It is computationally expensive usually, so not really worth it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIbu9Pwn44w6"
   },
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPYs2jXR8U0V"
   },
   "source": [
    "### 4.1. Performance Metrics [Asia]\n",
    "\n",
    "Compare all models used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gK4DwIyc8fgM"
   },
   "source": [
    "## 5. Explainable AI (XAI) Techniques / Tunning [Ema]\n",
    "- Shap\n",
    "- Lime\n",
    "- Saliency Map\n",
    "\n",
    "Focus on how our model works and focus on missclassifed instances to understand why sometimes he get it wrong\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgdSTdDLi6qU"
   },
   "source": [
    "### Bonus\n",
    "\n",
    "idk if it makes sense:\n",
    "\n",
    "Do u remember statistical problem in SDS where given a test with a certain probability of test result being real it calculate the real prob of having the disease with conditional probabilites ? Idk if we can do something similar here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO2EEZubnyPR"
   },
   "source": [
    "# Results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
